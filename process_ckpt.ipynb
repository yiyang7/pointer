{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "from tensorflow.python import pywrap_tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# file_name = \"/home/cs224u/pointer/log/test_exp2/train/\" + \"model.ckpt-0\"\n",
    "# reader = tf.train.NewCheckpointReader(file_name)\n",
    "# var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "\n",
    "# config = tf.ConfigProto(allow_soft_placement=True)\n",
    "# with tf.Session(config=config) as sess:\n",
    "#     # define the new is_training tensor\n",
    "#     embedding_doc = tf.random_normal([10, 128],dtype=tf.float32, name='embedding_doc')\n",
    "\n",
    "#     # now import the graph using the .meta file of the checkpoint\n",
    "#     saver = tf.train.import_meta_graph(\n",
    "#     file_name+\".meta\", input_map={'seq2seq/embedding/embedding_doc:0':embedding_doc})\n",
    "\n",
    "#     # restore all weights using the model checkpoint \n",
    "#     saver.restore(sess, file_name)\n",
    "\n",
    "#     # save updated graph and variables values\n",
    "#     saver.save(sess, file_name)\n",
    "    \n",
    "# finite = []\n",
    "# all_infnan = []\n",
    "# some_infnan = []\n",
    "\n",
    "# for key in sorted(var_to_shape_map.keys()):\n",
    "#     tensor = reader.get_tensor(key)\n",
    "#     print (\"key: \", key)\n",
    "#     print (tensor.shape)\n",
    "#     if np.all(np.isfinite(tensor)):\n",
    "#         finite.append(key)\n",
    "#     else:\n",
    "#         if not np.any(np.isfinite(tensor)):\n",
    "#             all_infnan.append(key)\n",
    "#         else:\n",
    "#             some_infnan.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvalue(ckpt):\n",
    "    reader = pywrap_tensorflow.NewCheckpointReader(ckpt)\n",
    "    var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "    value = {}\n",
    "    for key in var_to_shape_map:\n",
    "#         if \"Adagrad\" not in key:\n",
    "            value[key] = reader.get_tensor(key)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "(50000, 128)\n",
      "238410\n"
     ]
    }
   ],
   "source": [
    "ckpt = '/home/cs224u/pointer/log/pretrained_model_tf1.2.1/train/model-238410'\n",
    "v = getvalue(ckpt)\n",
    "print (len(v))\n",
    "\n",
    "print(v['seq2seq/embedding/embedding'].shape)\n",
    "print (v['global_step'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "(50000, 16)\n",
      "0\n",
      "dict_keys(['seq2seq/reduce_final_st/w_reduce_h/Adagrad', 'seq2seq/reduce_final_st/w_reduce_c/Adagrad', 'seq2seq/reduce_final_st/w_reduce_c', 'seq2seq/reduce_final_st/bias_reduce_c/Adagrad', 'seq2seq/output_projection/w/Adagrad', 'seq2seq/output_projection/w', 'seq2seq/reduce_final_st/w_reduce_h', 'seq2seq/output_projection/v/Adagrad', 'seq2seq/encoder/bidirectional_rnn/fw/lstm_cell/kernel', 'seq2seq/decoder/attention_decoder/attn_0/v/Adagrad', 'seq2seq/encoder/bidirectional_rnn/fw/lstm_cell/bias/Adagrad', 'seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Matrix/Adagrad', 'seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Matrix', 'seq2seq/reduce_final_st/bias_reduce_c', 'seq2seq/decoder/attention_decoder/Linear/Matrix', 'seq2seq/decoder/attention_decoder/Attention/Linear_1/Bias/Adagrad', 'seq2seq/reduce_final_st/bias_reduce_h', 'seq2seq/decoder/attention_decoder/Attention/Linear/Matrix', 'seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Matrix', 'global_step', 'seq2seq/decoder/attention_decoder/Attention/Linear/Matrix/Adagrad', 'seq2seq/decoder/attention_decoder/Attention/Linear_0/Bias', 'seq2seq/encoder/bidirectional_rnn/fw/lstm_cell/kernel/Adagrad', 'seq2seq/decoder/attention_decoder/Linear/Bias/Adagrad', 'seq2seq/decoder/attention_decoder/Attention/Linear_0/Bias/Adagrad', 'seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Bias/Adagrad', 'seq2seq/encoder/bidirectional_rnn/bw/lstm_cell/kernel/Adagrad', 'seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Bias', 'seq2seq/encoder/bidirectional_rnn/bw/lstm_cell/bias/Adagrad', 'seq2seq/decoder/attention_decoder/Linear/Bias', 'seq2seq/decoder/attention_decoder/W_h/Adagrad', 'seq2seq/decoder/attention_decoder/attn_1/v', 'seq2seq/decoder/attention_decoder/Attention/Linear_1/Bias', 'seq2seq/decoder/attention_decoder/attn_1/v/Adagrad', 'seq2seq/embedding/embedding/Adagrad', 'seq2seq/encoder/bidirectional_rnn/bw/lstm_cell/kernel', 'seq2seq/decoder/attention_decoder/W_h', 'seq2seq/decoder/attention_decoder/Linear/Matrix/Adagrad', 'seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Bias', 'seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Bias/Adagrad', 'seq2seq/decoder/attention_decoder/lstm_cell/kernel/Adagrad', 'seq2seq/output_projection/v', 'seq2seq/decoder/attention_decoder/attn_0/v', 'seq2seq/encoder/bidirectional_rnn/bw/lstm_cell/bias', 'seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Matrix/Adagrad', 'seq2seq/decoder/attention_decoder/lstm_cell/bias', 'seq2seq/decoder/attention_decoder/lstm_cell/bias/Adagrad', 'seq2seq/decoder/attention_decoder/lstm_cell/kernel', 'seq2seq/embedding/embedding', 'seq2seq/reduce_final_st/bias_reduce_h/Adagrad', 'seq2seq/encoder/bidirectional_rnn/fw/lstm_cell/bias'])\n"
     ]
    }
   ],
   "source": [
    "ckpt = '/home/cs224u/pointer/log/ckpt_multi_attn_exp/train/model.ckpt-0'\n",
    "v_attn = getvalue(ckpt)\n",
    "print (len(value))\n",
    "\n",
    "print(v_attn['seq2seq/embedding/embedding'].shape)\n",
    "print (v_attn['global_step'])\n",
    "print (v_attn.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "(50000, 16)\n",
      "0\n",
      "dict_keys(['seq2seq/reduce_final_st/w_reduce_c/Adagrad', 'seq2seq/reduce_final_st/w_reduce_c', 'seq2seq/reduce_final_st/bias_reduce_c/Adagrad', 'seq2seq/output_projection/w/Adagrad', 'seq2seq/output_projection/w', 'seq2seq/reduce_final_st/w_reduce_h', 'seq2seq/output_projection/v/Adagrad', 'seq2seq/encoder/bidirectional_rnn/fw/lstm_cell/kernel/Adagrad', 'seq2seq/encoder/bidirectional_rnn/fw/lstm_cell/kernel', 'seq2seq/reduce_final_st/w_reduce_h/Adagrad', 'seq2seq/decoder/attention_decoder/Linear_1/Bias/Adagrad', 'seq2seq/reduce_final_st/bias_reduce_c', 'seq2seq/decoder/attention_decoder/Linear/Matrix', 'seq2seq/decoder/attention_decoder/v', 'seq2seq/decoder/attention_decoder/W_h', 'seq2seq/decoder/attention_decoder/Linear/Matrix/Adagrad', 'seq2seq/encoder/bidirectional_rnn/fw/lstm_cell/bias/Adagrad', 'seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Matrix/Adagrad', 'seq2seq/decoder/attention_decoder/Linear_0/Bias', 'seq2seq/decoder/attention_decoder/calculate_pgen/Linear_1/Bias', 'seq2seq/decoder/attention_decoder/lstm_cell/kernel', 'seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Matrix', 'seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Bias/Adagrad', 'seq2seq/decoder/attention_decoder/Attention/Linear/Bias', 'seq2seq/encoder/bidirectional_rnn/bw/lstm_cell/kernel/Adagrad', 'seq2seq/reduce_final_st/bias_reduce_h', 'seq2seq/decoder/attention_decoder/Attention/Linear/Matrix', 'seq2seq/decoder/attention_decoder/Linear_0/Bias/Adagrad', 'seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Matrix', 'seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Bias', 'seq2seq/encoder/bidirectional_rnn/bw/lstm_cell/bias/Adagrad', 'seq2seq/decoder/attention_decoder/Attention/Linear/Bias/Adagrad', 'seq2seq/decoder/attention_decoder/lstm_cell/kernel/Adagrad', 'seq2seq/output_projection/v', 'seq2seq/encoder/bidirectional_rnn/bw/lstm_cell/bias', 'seq2seq/decoder/attention_decoder/Attention/Linear/Matrix/Adagrad', 'seq2seq/decoder/attention_decoder/W_h/Adagrad', 'seq2seq/decoder/attention_decoder/Linear_1/Bias', 'seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Matrix/Adagrad', 'seq2seq/decoder/attention_decoder/calculate_pgen/Linear_0/Bias/Adagrad', 'global_step', 'seq2seq/decoder/attention_decoder/calculate_pgen/Linear_1/Bias/Adagrad', 'seq2seq/decoder/attention_decoder/lstm_cell/bias', 'seq2seq/decoder/attention_decoder/lstm_cell/bias/Adagrad', 'seq2seq/decoder/attention_decoder/v/Adagrad', 'seq2seq/decoder/attention_decoder/calculate_pgen/Linear_0/Bias', 'seq2seq/embedding/embedding', 'seq2seq/embedding/embedding/Adagrad', 'seq2seq/encoder/bidirectional_rnn/bw/lstm_cell/kernel', 'seq2seq/reduce_final_st/bias_reduce_h/Adagrad', 'seq2seq/encoder/bidirectional_rnn/fw/lstm_cell/bias'])\n"
     ]
    }
   ],
   "source": [
    "ckpt = '/home/cs224u/pointer/log/ckpt_multi_pgen_exp/train/model.ckpt-0'\n",
    "v_pgen = getvalue(ckpt)\n",
    "print (len(value))\n",
    "\n",
    "print(v_pgen['seq2seq/embedding/embedding'].shape)\n",
    "print (v_pgen['global_step'])\n",
    "print (v_pgen.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq/decoder/attention_decoder/attn_0/v/Adagrad\n",
      "seq2seq/decoder/attention_decoder/Attention/Linear_1/Bias/Adagrad\n",
      "seq2seq/decoder/attention_decoder/Attention/Linear_0/Bias\n",
      "seq2seq/decoder/attention_decoder/Linear/Bias/Adagrad\n",
      "seq2seq/decoder/attention_decoder/Attention/Linear_0/Bias/Adagrad\n",
      "seq2seq/decoder/attention_decoder/Linear/Bias\n",
      "seq2seq/decoder/attention_decoder/attn_1/v\n",
      "seq2seq/decoder/attention_decoder/Attention/Linear_1/Bias\n",
      "seq2seq/decoder/attention_decoder/attn_1/v/Adagrad\n",
      "seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Bias\n",
      "seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Bias/Adagrad\n",
      "seq2seq/decoder/attention_decoder/attn_0/v\n",
      "---\n",
      "seq2seq/decoder/attention_decoder/Linear_1/Bias/Adagrad\n",
      "seq2seq/decoder/attention_decoder/v\n",
      "seq2seq/decoder/attention_decoder/Linear_0/Bias\n",
      "seq2seq/decoder/attention_decoder/calculate_pgen/Linear_1/Bias\n",
      "seq2seq/decoder/attention_decoder/Attention/Linear/Bias\n",
      "seq2seq/decoder/attention_decoder/Linear_0/Bias/Adagrad\n",
      "seq2seq/decoder/attention_decoder/Attention/Linear/Bias/Adagrad\n",
      "seq2seq/decoder/attention_decoder/Linear_1/Bias\n",
      "seq2seq/decoder/attention_decoder/calculate_pgen/Linear_0/Bias/Adagrad\n",
      "seq2seq/decoder/attention_decoder/calculate_pgen/Linear_1/Bias/Adagrad\n",
      "seq2seq/decoder/attention_decoder/v/Adagrad\n",
      "seq2seq/decoder/attention_decoder/calculate_pgen/Linear_0/Bias\n"
     ]
    }
   ],
   "source": [
    "for k in v_attn.keys():\n",
    "    if k not in v_pgen:\n",
    "        print (k)\n",
    "        \n",
    "print (\"---\")\n",
    "\n",
    "for k in v_pgen.keys():\n",
    "    if k not in v_attn:\n",
    "        print (k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'seq2seq/decoder/attention_decoder/Linear_0/Bias'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu3",
   "language": "python",
   "name": "nlu3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
