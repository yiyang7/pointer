{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_util import *\n",
    "from random import sample\n",
    "\n",
    "SubReddits_to_include_10 = ['relationships', 'legaladvice', 'nfl',  'pettyrevenge', 'atheismbot', 'ShouldIbuythisgame', 'ukpolitics', 'Dogtraining',  'AskHistorians', 'Anxiety']\n",
    "\n",
    "\n",
    "# read jsonl dataset\n",
    "src = \"/home/ubuntu/cs224u/raw_reddit/tldr-training-data.jsonl\"\n",
    "reader = jsonlines.open(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessed_story_file_include_subreddit(input_dict, save_dir):\n",
    "    '''\n",
    "    input:\n",
    "    input_dict: input dictionary, include information about its id, content, summary etc\n",
    "    save_dir: a directory about where to save the story files\n",
    "    reference: https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
    "    here we preprocessed the content and the summary of the story by:\n",
    "    1) get rid of extra space tab\n",
    "    2) filter out those whose summary is too short/content is too short\n",
    "    3) delete special characters like [...]\n",
    "    4) [potential] Stemming (spend/spent/spends...)\n",
    "    5) [potential] Lemmatization (do/done/did)\n",
    "    '''\n",
    "    dic_id = input_dict[\"id\"]\n",
    "    content = input_dict[\"content\"]\n",
    "    summary = input_dict['summary']\n",
    "    subreddit = input_dict[\"subreddit\"]\n",
    "    #print(type(summary.split()))\n",
    "    if(len(summary.split()) > 3):\n",
    "        # get rid of extra space tab\n",
    "        content = re.sub('\\s+', ' ', content).strip()\n",
    "        summary = re.sub('\\s+', ' ', summary).strip()    \n",
    "        # get rid of words inside special characterss\n",
    "        content = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", content)\n",
    "        summary = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", summary)\n",
    "\n",
    "        filename = os.path.join(save_dir, dic_id +'_'+subreddit+ \".story\")\n",
    "        file1 = open(filename,\"w\")\n",
    "        # add the subreddit information before the summary\n",
    "        file1.writelines(content+'\\n')\n",
    "        file1.writelines('@highlight \\n')\n",
    "        file1.writelines(subreddit+' '+summary)\n",
    "        file1.close()\n",
    "        \n",
    "        \n",
    "def create_final_list(subreddit_type, input_type, input_str):\n",
    "    filename = os.path.join(\"/home/ubuntu/cs224u/processed_10_1k\"+'/'+\"processed_\"+subreddit_type+\"/\", subreddit_type + input_type + \"_list.txt\")\n",
    "    print(filename)\n",
    "    f = open(filename,\"w\")\n",
    "    f.writelines(input_str)\n",
    "    f.close()          \n",
    "       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/cs224u/processed_10_1k/processed_ukpolitics/ukpolitics_list.txt'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit_type = \"ukpolitics\"\n",
    "f1 = os.path.join(\"/home/ubuntu/cs224u/processed_10_1k\"+'/'+\"processed_\"+subreddit_type+\"/\", subreddit_type  + \"list.txt\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for i in range(len(SubReddits_to_include_10)):\n",
    "        print(i)\n",
    "        # choose a subreddit\n",
    "        input_subreddit = SubReddits_to_include_10[i]\n",
    "        # create the directory to this corresponding dataset\n",
    "        dst = \"/home/ubuntu/cs224u/processed_10_1k\"+'/'+\"processed_\"+input_subreddit\n",
    "        os.mkdir(dst)\n",
    "        dst = \"/home/ubuntu/cs224u/processed_10_1k\"+'/'+\"processed_\"+input_subreddit+\"/\"+input_subreddit+'_story'\n",
    "        os.mkdir(dst)\n",
    "        # get a corresponding dataset\n",
    "        count = 0\n",
    "        dic_list = []\n",
    "        reader = jsonlines.open(src)\n",
    "        for dic in reader:\n",
    "            if(\"subreddit\" in dic.keys() and dic[\"subreddit\"] == input_subreddit and \n",
    "               isEnglish(dic[\"summary\"]) == True and  isEnglish(dic[\"content\"]) == True):\n",
    "                dic_list.append(dic)\n",
    "\n",
    "        # create a small dataset if needed\n",
    "        sample_list = sample(dic_list,1200)\n",
    "        for dic in sample_list:\n",
    "            create_preprocessed_story_file_include_subreddit(dic, dst)  \n",
    "\n",
    "        #input_subreddit = \"relationships\"\n",
    "        #dst = \"/home/ubuntu/cs224u/new_relationships\"+'/'+input_subreddit+'_story'\n",
    "        result_list = os.listdir(dst)\n",
    "        np.random.shuffle(result_list)\n",
    "        size = len(result_list)\n",
    "\n",
    "        train_list = result_list[0:int(0.8*size)-1]\n",
    "        train_str = \"\\n\".join(x for x in train_list)\n",
    "\n",
    "        dev_list = result_list[int(0.8*size):int(0.9*size)-1]\n",
    "        dev_str = \"\\n\".join(x for x in dev_list)\n",
    "\n",
    "        test_list = result_list[int(0.9*size): int(size)-1]\n",
    "        test_str = \"\\n\".join(x for x in test_list)\n",
    "        # create three lists\n",
    "        create_final_list(input_subreddit, \"_train\", train_str)\n",
    "        create_final_list(input_subreddit, \"_val\", dev_str)\n",
    "        create_final_list(input_subreddit, \"_test\", test_str)\n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read jsonl dataset\n",
    "src = \"/home/ubuntu/cs224u/raw_reddit/tldr-training-data.jsonl\"\n",
    "reader = jsonlines.open(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SubReddits_to_include_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(SubReddits_to_include_10)):\n",
    "    print(i)\n",
    "    # choose a subreddit\n",
    "    input_subreddit = SubReddits_to_include_10[i]\n",
    "    # create the directory to this corresponding dataset\n",
    "    dst = \"/home/ubuntu/cs224u/processesd_10_1k\"+'/'+input_subreddit+'_story'\n",
    "    #os.mkdir(dst)\n",
    "    # get a corresponding dataset\n",
    "    count = 0\n",
    "    dic_list = []\n",
    "    reader = jsonlines.open(src)\n",
    "    for dic in reader:\n",
    "        if(\"subreddit\" in dic.keys() and dic[\"subreddit\"] == input_subreddit and \n",
    "           isEnglish(dic[\"summary\"]) == True and  isEnglish(dic[\"content\"]) == True):\n",
    "            dic_list.append(dic)\n",
    "            \n",
    "    # create a small dataset if needed\n",
    "    sample_list = sample(dic_list,1200)\n",
    "    for dic in sample_list:\n",
    "        create_preprocessed_story_file(dic, dst)  \n",
    "\n",
    "    #input_subreddit = \"relationships\"\n",
    "    #dst = \"/home/ubuntu/cs224u/new_relationships\"+'/'+input_subreddit+'_story'\n",
    "    result_list = os.listdir(dst)\n",
    "    np.random.shuffle(result_list)\n",
    "    size = len(result_list)\n",
    "\n",
    "    train_list = result_list[0:int(0.8*size)-1]\n",
    "    train_str = \"\\n\".join(x for x in train_list)\n",
    "\n",
    "    dev_list = result_list[int(0.8*size):int(0.9*size)-1]\n",
    "    dev_str = \"\\n\".join(x for x in dev_list)\n",
    "\n",
    "    test_list = result_list[int(0.9*size): int(size)-1]\n",
    "    test_str = \"\\n\".join(x for x in test_list)\n",
    "    # create three lists\n",
    "    create_final_list(input_subreddit, \"_train\", train_str)\n",
    "    create_final_list(input_subreddit, \"_val\", dev_str)\n",
    "    create_final_list(input_subreddit, \"_test\", test_str)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "input_subreddit = \"ukpolitics\"\n",
    "# create the directory to this corresponding dataset\n",
    "dst = \"/home/ubuntu/cs224u/processed_10_1k\"+'/'+\"processed_\"+input_subreddit\n",
    "os.mkdir(dst)\n",
    "dst = \"/home/ubuntu/cs224u/processed_10_1k\"+'/'+\"processed_\"+input_subreddit+\"/\"+input_subreddit+'_story'\n",
    "os.mkdir(dst)\n",
    "# get a corresponding dataset\n",
    "count = 0\n",
    "dic_list = []\n",
    "reader = jsonlines.open(src)\n",
    "for dic in reader:\n",
    "    if(\"subreddit\" in dic.keys() and dic[\"subreddit\"] == input_subreddit and \n",
    "       isEnglish(dic[\"summary\"]) == True and  isEnglish(dic[\"content\"]) == True):\n",
    "        dic_list.append(dic)\n",
    "\n",
    "# create a small dataset if needed\n",
    "sample_list = sample(dic_list,1200)\n",
    "for dic in sample_list:\n",
    "    create_preprocessed_story_file_include_subreddit(dic, dst)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/cs224u/processed_10_1k/processed_ukpolitics/ukpolitics_train_list.txt\n",
      "/home/ubuntu/cs224u/processed_10_1k/processed_ukpolitics/ukpolitics_val_list.txt\n",
      "/home/ubuntu/cs224u/processed_10_1k/processed_ukpolitics/ukpolitics_test_list.txt\n"
     ]
    }
   ],
   "source": [
    "#input_subreddit = \"relationships\"\n",
    "#dst = \"/home/ubuntu/cs224u/new_relationships\"+'/'+input_subreddit+'_story'\n",
    "result_list = os.listdir(dst)\n",
    "np.random.shuffle(result_list)\n",
    "size = len(result_list)\n",
    "\n",
    "train_list = result_list[0:int(0.8*size)-1]\n",
    "train_str = \"\\n\".join(x for x in train_list)\n",
    "\n",
    "dev_list = result_list[int(0.8*size):int(0.9*size)-1]\n",
    "dev_str = \"\\n\".join(x for x in dev_list)\n",
    "\n",
    "test_list = result_list[int(0.9*size): int(size)-1]\n",
    "test_str = \"\\n\".join(x for x in test_list)\n",
    "# create three lists\n",
    "create_final_list(input_subreddit, \"_train\", train_str)\n",
    "create_final_list(input_subreddit, \"_val\", dev_str)\n",
    "create_final_list(input_subreddit, \"_test\", test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessed_story_file_include_subreddit(input_dict, save_dir):\n",
    "    '''\n",
    "    input:\n",
    "    input_dict: input dictionary, include information about its id, content, summary etc\n",
    "    save_dir: a directory about where to save the story files\n",
    "    reference: https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
    "    here we preprocessed the content and the summary of the story by:\n",
    "    1) get rid of extra space tab\n",
    "    2) filter out those whose summary is too short/content is too short\n",
    "    3) delete special characters like [...]\n",
    "    4) [potential] Stemming (spend/spent/spends...)\n",
    "    5) [potential] Lemmatization (do/done/did)\n",
    "    '''\n",
    "    dic_id = input_dict[\"id\"]\n",
    "    content = input_dict[\"content\"]\n",
    "    summary = input_dict['summary']\n",
    "    subreddit = input_dict[\"subreddit\"]\n",
    "    if(summary.split() > 3):\n",
    "        # get rid of extra space tab\n",
    "        content = re.sub('\\s+', ' ', content).strip()\n",
    "        summary = re.sub('\\s+', ' ', summary).strip()    \n",
    "        # get rid of words inside special characterss\n",
    "        content = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", content)\n",
    "        summary = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", summary)\n",
    "\n",
    "        filename = os.path.join(save_dir, dic_id +subreddit+ \".story\")\n",
    "        file1 = open(filename,\"w\")\n",
    "        # add the subreddit information before the summary\n",
    "        file1.writelines(content+'\\n')\n",
    "        file1.writelines('@highlight \\n')\n",
    "        file1.writelines(subreddit+summary)\n",
    "        file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_list(subreddit_type, input_type, input_str):\n",
    "    filename = os.path.join(\"/home/ubuntu/cs224u/processesd_10_1k/\"+ subreddit_type+\"/\", subreddit_type + input_type + \"list.txt\")\n",
    "    print(filename)\n",
    "    f = open(filename,\"w\")\n",
    "    f.writelines(input_str)\n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a corresponding dataset\n",
    "count = 0\n",
    "dic_list = []\n",
    "for dic in reader:\n",
    "    if(\"subreddit\" in dic.keys() and dic[\"subreddit\"] == input_subreddit and \n",
    "       isEnglish(dic[\"summary\"]) == True and  isEnglish(dic[\"content\"]) == True ):\n",
    "        dic_list.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a small dataset if needed\n",
    "sample_list = sample(dic_list,1200)\n",
    "for dic in sample_list:\n",
    "    create_preprocessed_story_file(dic, dst)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dst_whole = \"/home/ubuntu/cs224u/processed_politics\"+'/'+input_subreddit+'_story_whole'\n",
    "#os.mkdir(dst_whole)\n",
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the whole dataset \n",
    "#sample_list = sample(dic_list,100)\n",
    "for dic in dic_list:\n",
    "    create_preprocessed_story_file(dic, dst)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get corresponding list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_subreddit = \"relationships\"\n",
    "#dst = \"/home/ubuntu/cs224u/new_relationships\"+'/'+input_subreddit+'_story'\n",
    "result_list = os.listdir(dst)\n",
    "np.random.shuffle(result_list)\n",
    "size = len(result_list)\n",
    "\n",
    "train_list = result_list[0:int(0.8*size)-1]\n",
    "train_str = \"\\n\".join(x for x in train_list)\n",
    "\n",
    "dev_list = result_list[int(0.8*size):int(0.9*size)-1]\n",
    "dev_str = \"\\n\".join(x for x in dev_list)\n",
    "\n",
    "test_list = result_list[int(0.9*size): int(size)-1]\n",
    "test_str = \"\\n\".join(x for x in test_list)\n",
    "\n",
    "# print (len(train_list))\n",
    "# print (len(dev_list))\n",
    "# print (len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create three lists\n",
    "create_list(input_subreddit, \"_train\", train_str)\n",
    "create_list(input_subreddit, \"_val\", dev_str)\n",
    "create_list(input_subreddit, \"_test\", test_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a baseline result for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the directory to the corresponding baseline result\n",
    "make_dir = '/home/ubuntu/cs224u/processed_' +input_subreddit+'/baseline' \n",
    "os.mkdir(make_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dec_dir = make_dir + '/decoded'\n",
    "os.mkdir(make_dec_dir)\n",
    "\n",
    "make_ref_dir = make_dir + '/reference'\n",
    "os.mkdir(make_ref_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the name of the test list\n",
    "test_name_list = [x[:-6] for x in test_list]\n",
    "test_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = jsonlines.open(src)\n",
    "# create corresponding baseline summarization\n",
    "for dic in reader:\n",
    "    if(\"subreddit\" in dic.keys() and dic[\"subreddit\"] == input_subreddit and isEnglish(dic[\"content\"]) == True):\n",
    "        if(dic[\"id\"] in test_name_list):\n",
    "            print(dic[\"id\"])\n",
    "            create_baseline_summarization_file(dic, make_dec_dir)\n",
    "            create_reference_file(dic, make_ref_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create an example.story (not relavant to here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dic in reader:\n",
    "    if(\"subreddit\" in dic.keys() and dic[\"subreddit\"] == \"AskReddit\" and isEnglish(dic[\"summary\"]) == True and  isEnglish(dic[\"content\"]) == True ):\n",
    "        print(dic)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dic[\"summary\"].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cwd = os.getcwd()\n",
    "filename = os.path.join(cwd, \"example.story\")\n",
    "file1 = open(filename,\"w\")\n",
    "file1.writelines(dic[\"content\"]+'\\n')\n",
    "#file1.writelines('@hightlight \\n')\n",
    "#file1.writelines(input_dict['summary'])\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic[\"summary\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
