{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_from_disks(glove_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a GloVe txt file. If `with_indexes=True`, we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    \n",
    "    with open(glove_filename, 'r') as glove_file:\n",
    "        for (i, line) in enumerate(glove_file):\n",
    "            \n",
    "            split = line.split(' ')\n",
    "            \n",
    "            word = split[0]\n",
    "            \n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "            \n",
    "            if with_indexes:\n",
    "                if word in word_to_index_dict:\n",
    "                    print (\"dup word: \", word)\n",
    "                else:\n",
    "                    word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "    \n",
    "    print (\"load_embedding_from_disks representation: \", len(representation))\n",
    "    \n",
    "    _WORD_NOT_FOUND = [0.01]* len(representation)  # Empty representation for unknown words.\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        print (\"index_to_embedding_array: \", len(index_to_embedding_array))\n",
    "        index_to_embedding_array = np.array(index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        print (\"word_to_index_dict: \", len(word_to_index_dict))\n",
    "        print (\"index_to_embedding_array: \", index_to_embedding_array.shape)\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dup word:  <unk>\n",
      "dup word:  <unk>\n",
      "load_embedding_from_disks representation:  50\n",
      "index_to_embedding_array:  1193517\n",
      "word_to_index_dict:  1193515\n",
      "index_to_embedding_array:  (1193518, 50)\n"
     ]
    }
   ],
   "source": [
    "word_to_index_dict, index_to_embedding_array = load_embedding_from_disks(\"/home/ubuntu/cs224u/glove/glove.twitter.27B.50d.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ubuntu/cs224u/processed_10_1k_mymodel/processed_combine_all/combine_all_story/\"\n",
    "stories = os.listdir(path)\n",
    "word2id, id2emb = dict(), np.zeros((len(word_to_index_dict),50))\n",
    "count = 0\n",
    "for s in stories:\n",
    "    f = open(path+s,\"r\")\n",
    "    txt = \"\"\n",
    "    for i in f.readlines():\n",
    "        txt += i \n",
    "    for p in txt.split(\"\\n\"):\n",
    "        p = p.translate(str.maketrans('', '', string.punctuation))\n",
    "        for w in p.split():\n",
    "            ind = word_to_index_dict[w]\n",
    "            emb = index_to_embedding_array[ind]\n",
    "            if w not in word2id:\n",
    "                word2id[w] = count\n",
    "                id2emb[count,:] = emb\n",
    "                count += 1\n",
    "id2emb = id2emb[:len(word2id)]\n",
    "_WORD_NOT_FOUND = [0.01]* len(representation)\n",
    "_LAST_INDEX = len(word2id)\n",
    "word2id = defaultdict(lambda: _LAST_INDEX, word2id)\n",
    "id2emb = id2emb + np.array(_WORD_NOT_FOUND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114527"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu3",
   "language": "python",
   "name": "nlu3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
