{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_util import *\n",
    "from random import sample\n",
    "\n",
    "SubReddits_to_include_10 = ['relationships', 'legaladvice', 'nfl',  'pettyrevenge', 'atheismbot', 'ShouldIbuythisgame', 'ukpolitics', 'Dogtraining',  'AskHistorians', 'Anxiety']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relationships',\n",
       " 'legaladvice',\n",
       " 'nfl',\n",
       " 'pettyrevenge',\n",
       " 'atheismbot',\n",
       " 'ShouldIbuythisgame',\n",
       " 'ukpolitics',\n",
       " 'Dogtraining',\n",
       " 'AskHistorians',\n",
       " 'Anxiety']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SubReddits_to_include_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_content_for_subreddit(input_subreddit):\n",
    "    long_content_i = str()\n",
    "    # create the directory to this corresponding dataset\n",
    "    dst = \"/home/ubuntu/cs224u/processed_10_1k\"+'/'+ \"processed_\"+input_subreddit+\"/\"+input_subreddit+'_story'\n",
    "    for filename in os.listdir(dst):\n",
    "        if filename.endswith(\".story\"):\n",
    "            with open(dst+\"/\"+filename, \"r\") as myfile:\n",
    "                data = str(myfile.readlines())\n",
    "                data = data.replace(\"[\",\"\")\n",
    "                data = data.replace(\"]\",\"\")\n",
    "                # extract the sentence before \\n@highlight\n",
    "                ext = \"@highlight\"\n",
    "                content = data[:data.find(ext)-6]\n",
    "\n",
    "                # append the content together and form a long string\n",
    "                long_content_i = long_content_i + content\n",
    "    return long_content_i\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst = \"/home/ubuntu/cs224u/processed_10_1k\"\n",
    "with open(dst+\"/\"+\"doc2vec_all_subreddit.csv\", \"a\") as myfile:\n",
    "    myfile.writelines(\"test\\n\")\n",
    "    for input_subreddit in SubReddits_to_include_10:\n",
    "        myfile.writelines(create_content_for_subreddit(input_subreddit))\n",
    "        myfile.writelines(\"\\n\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a corresponding dataset\n",
    "count = 0\n",
    "dic_list = []\n",
    "for dic in reader:\n",
    "    if(\"subreddit\" in dic.keys() and dic[\"subreddit\"] == input_subreddit and \n",
    "       isEnglish(dic[\"summary\"]) == True and  isEnglish(dic[\"content\"]) == True ):\n",
    "        dic_list.append(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a small dataset if needed\n",
    "sample_list = sample(dic_list,1200)\n",
    "for dic in sample_list:\n",
    "    create_preprocessed_story_file(dic, dst)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the whole dataset \n",
    "#sample_list = sample(dic_list,100)\n",
    "for dic in dic_list:\n",
    "    create_preprocessed_story_file(dic, dst)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine dataset and lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all data into a file\n",
    "root_dir = \"/home/ubuntu/cs224u/processed_10_1k\"\n",
    "# create a processed_combine_all file\n",
    "dst = \"/home/ubuntu/cs224u/processed_10_1k\"+'/'+\"processed_combine_all\"\n",
    "#os.mkdir(dst)\n",
    "dest = \"/home/ubuntu/cs224u/processed_10_1k\"+'/'+\"processed_combine_all/\"+ \"combine_all_story\"\n",
    "#os.mkdir(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(SubReddits_to_include_10)):\n",
    "    # add all information into the processed_combine_all file from \n",
    "    input_subreddit = SubReddits_to_include_10[i]\n",
    "    src = root_dir + \"/processed_\"+ input_subreddit + \"/\"+input_subreddit + \"_story\"\n",
    "    src_files = os.listdir(src)\n",
    "    for file_name in src_files:\n",
    "        full_file_name = os.path.join(src, file_name)\n",
    "        if os.path.isfile(full_file_name):\n",
    "            shutil.copy(full_file_name, dest)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/cs224u/processed_10_1k/processed_combine_all'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/cs224u/processed_10_1k/processed_combine_all/combine_train_list.txt\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_relationships/relationships_train_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_legaladvice/legaladvice_train_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_nfl/nfl_train_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_pettyrevenge/pettyrevenge_train_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_atheismbot/atheismbot_train_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_ShouldIbuythisgame/ShouldIbuythisgame_train_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_ukpolitics/ukpolitics_train_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_Dogtraining/Dogtraining_train_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_AskHistorians/AskHistorians_train_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_Anxiety/Anxiety_train_list.txt']\n"
     ]
    }
   ],
   "source": [
    "# combine all subreddit_train_list.txt data into a combine_train_list.txt\n",
    "# create a result outfile name combine_train_list.txt\n",
    "# combine_train_list.txt\n",
    "\n",
    "combine_train = os.path.join(dst, \"combine_train_list.txt\")\n",
    "print(combine_train)\n",
    "\n",
    "#\"/home/ubuntu/cs224u/processed_10_1k/processed_\" + input_subreddit+ \"/\"+input_subreddit+\"_train_list.txt\"\n",
    "\n",
    "for i in range(len(SubReddits_to_include_10)):\n",
    "    # add all information into the processed_combine_all file from \n",
    "    input_subreddit = SubReddits_to_include_10[i]\n",
    "    read_files = glob.glob(\"/home/ubuntu/cs224u/processed_10_1k/processed_\" + input_subreddit+ \"/\"+input_subreddit+\"_train_list.txt\")\n",
    "    print(read_files)\n",
    "    combine_train = os.path.join(dst, \"combine_train_list.txt\")\n",
    "    with open(combine_train, \"a\") as outfile:\n",
    "        for f in read_files:\n",
    "            with open(f, \"rb\") as infile:\n",
    "                outfile.writelines(infile.read())\n",
    "            outfile.writelines(\"\\n\")\n",
    "      #  num_lines = sum(1 for line in open(\"/home/ubuntu/cs224u/processed_10_1k/processed_combine_all/combine_all_story/combine_train_list.txt\"))\n",
    "      #  print(num_lines)\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/cs224u/processed_10_1k/processed_combine_all/combine_val_list.txt\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_relationships/relationships_val_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_legaladvice/legaladvice_val_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_nfl/nfl_val_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_pettyrevenge/pettyrevenge_val_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_atheismbot/atheismbot_val_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_ShouldIbuythisgame/ShouldIbuythisgame_val_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_ukpolitics/ukpolitics_val_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_Dogtraining/Dogtraining_val_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_AskHistorians/AskHistorians_val_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_Anxiety/Anxiety_val_list.txt']\n"
     ]
    }
   ],
   "source": [
    "# combine all subreddit_train_list.txt data into a combine_train_list.txt\n",
    "# create a result outfile name combine_train_list.txt\n",
    "# combine_train_list.txt\n",
    "\n",
    "combine_train = os.path.join(dst, \"combine_val_list.txt\")\n",
    "print(combine_train)\n",
    "\n",
    "#\"/home/ubuntu/cs224u/processed_10_1k/processed_\" + input_subreddit+ \"/\"+input_subreddit+\"_train_list.txt\"\n",
    "\n",
    "for i in range(len(SubReddits_to_include_10)):\n",
    "    # add all information into the processed_combine_all file from \n",
    "    input_subreddit = SubReddits_to_include_10[i]\n",
    "    read_files = glob.glob(\"/home/ubuntu/cs224u/processed_10_1k/processed_\" + input_subreddit+ \"/\"+input_subreddit+\"_val_list.txt\")\n",
    "    print(read_files)\n",
    "    combine_train = os.path.join(dst, \"combine_val_list.txt\")\n",
    "    with open(combine_train, \"a\") as outfile:\n",
    "        for f in read_files:\n",
    "            with open(f, \"rb\") as infile:\n",
    "                outfile.writelines(infile.read())\n",
    "            outfile.writelines(\"\\n\")\n",
    "     #   num_lines = sum(1 for line in open(\"/home/ubuntu/cs224u/processed_10_1k/processed_combine_all/combine_all_story/combine_val_list.txt\"))\n",
    "     #   print(num_lines)\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/cs224u/processed_10_1k/processed_combine_all/combine_test_list.txt\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_relationships/relationships_test_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_legaladvice/legaladvice_test_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_nfl/nfl_test_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_pettyrevenge/pettyrevenge_test_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_atheismbot/atheismbot_test_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_ShouldIbuythisgame/ShouldIbuythisgame_test_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_ukpolitics/ukpolitics_test_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_Dogtraining/Dogtraining_test_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_AskHistorians/AskHistorians_test_list.txt']\n",
      "['/home/ubuntu/cs224u/processed_10_1k/processed_Anxiety/Anxiety_test_list.txt']\n"
     ]
    }
   ],
   "source": [
    "# combine all subreddit_train_list.txt data into a combine_train_list.txt\n",
    "# create a result outfile name combine_train_list.txt\n",
    "# combine_train_list.txt\n",
    "\n",
    "combine_train = os.path.join(dst, \"combine_test_list.txt\")\n",
    "print(combine_train)\n",
    "\n",
    "#\"/home/ubuntu/cs224u/processed_10_1k/processed_\" + input_subreddit+ \"/\"+input_subreddit+\"_train_list.txt\"\n",
    "\n",
    "for i in range(len(SubReddits_to_include_10)):\n",
    "    # add all information into the processed_combine_all file from \n",
    "    input_subreddit = SubReddits_to_include_10[i]\n",
    "    read_files = glob.glob(\"/home/ubuntu/cs224u/processed_10_1k/processed_\" + input_subreddit+ \"/\"+input_subreddit+\"_test_list.txt\")\n",
    "    print(read_files)\n",
    "    combine_train = os.path.join(dst, \"combine_test_list.txt\")\n",
    "    with open(combine_train, \"a\") as outfile:\n",
    "        for f in read_files:\n",
    "            with open(f, \"rb\") as infile:\n",
    "                outfile.writelines(infile.read())\n",
    "        outfile.writelines(\"\\n\")\n",
    "    #    num_lines = sum(1 for line in open(\"/home/ubuntu/cs224u/processed_10_1k/processed_combine_all/combine_all_story/combine_test_list.txt\"))\n",
    "    #    print(num_lines)\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a baseline result for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the directory to the corresponding baseline result\n",
    "make_dir = '/home/ubuntu/cs224u/processed_' +input_subreddit+'/baseline' \n",
    "os.mkdir(make_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dec_dir = make_dir + '/decoded'\n",
    "os.mkdir(make_dec_dir)\n",
    "\n",
    "make_ref_dir = make_dir + '/reference'\n",
    "os.mkdir(make_ref_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the name of the test list\n",
    "test_name_list = [x[:-6] for x in test_list]\n",
    "test_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = jsonlines.open(src)\n",
    "# create corresponding baseline summarization\n",
    "for dic in reader:\n",
    "    if(\"subreddit\" in dic.keys() and dic[\"subreddit\"] == input_subreddit and isEnglish(dic[\"content\"]) == True):\n",
    "        if(dic[\"id\"] in test_name_list):\n",
    "            print(dic[\"id\"])\n",
    "            create_baseline_summarization_file(dic, make_dec_dir)\n",
    "            create_reference_file(dic, make_ref_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create an example.story (not relavant to here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dic in reader:\n",
    "    if(\"subreddit\" in dic.keys() and dic[\"subreddit\"] == \"AskReddit\" and isEnglish(dic[\"summary\"]) == True and  isEnglish(dic[\"content\"]) == True ):\n",
    "        print(dic)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dic[\"summary\"].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cwd = os.getcwd()\n",
    "filename = os.path.join(cwd, \"example.story\")\n",
    "file1 = open(filename,\"w\")\n",
    "file1.writelines(dic[\"content\"]+'\\n')\n",
    "#file1.writelines('@hightlight \\n')\n",
    "#file1.writelines(input_dict['summary'])\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic[\"summary\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
