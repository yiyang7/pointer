{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "from tensorflow.python import pywrap_tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"/home/cs224u/pointer/log/test_exp2/train/\" + \"model.ckpt-0\"\n",
    "reader = tf.train.NewCheckpointReader(file_name)\n",
    "var_to_shape_map = reader.get_variable_to_shape_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "with tf.Session(config=config) as sess:\n",
    "    # define the new is_training tensor\n",
    "    embedding_doc = tf.random_normal([10, 128],dtype=tf.float32, name='embedding_doc')\n",
    "\n",
    "    # now import the graph using the .meta file of the checkpoint\n",
    "    saver = tf.train.import_meta_graph(\n",
    "    file_name+\".meta\", input_map={'seq2seq/embedding/embedding_doc:0':embedding_doc})\n",
    "\n",
    "    # restore all weights using the model checkpoint \n",
    "    saver.restore(sess, file_name)\n",
    "\n",
    "    # save updated graph and variables values\n",
    "    saver.save(sess, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finite = []\n",
    "all_infnan = []\n",
    "some_infnan = []\n",
    "\n",
    "for key in sorted(var_to_shape_map.keys()):\n",
    "    tensor = reader.get_tensor(key)\n",
    "    print (\"key: \", key)\n",
    "    print (tensor.shape)\n",
    "    if np.all(np.isfinite(tensor)):\n",
    "        finite.append(key)\n",
    "    else:\n",
    "        if not np.any(np.isfinite(tensor)):\n",
    "            all_infnan.append(key)\n",
    "        else:\n",
    "            some_infnan.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvalue(ckpt):\n",
    "    reader = pywrap_tensorflow.NewCheckpointReader(ckpt)\n",
    "    var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "    value = {}\n",
    "    for key in var_to_shape_map:\n",
    "#         if \"Adagrad\" not in key:\n",
    "            value[key] = reader.get_tensor(key)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "# get value from checkpoint\n",
    "ckpt = '/home/cs224u/pointer/log/pretrained_model_tf1.2.1/train/model-238410'\n",
    "value1 = getvalue(ckpt)\n",
    "print (len(value1))\n",
    "\n",
    "# ckpt = '/home/cs224u/pointer/log/docvec_ckpt_exp/train/model.ckpt-1'\n",
    "ckpt = '/home/cs224u/pointer/log/load_word_emb_doc_vec_combine_exp/train/model.ckpt-0'\n",
    "value2 = getvalue(ckpt)\n",
    "print (len(value2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.8002606e-02  1.8546827e-02  1.3938937e-01 ... -4.9497869e-02\n",
      "   3.3019509e-02  3.5647683e-02]\n",
      " [ 7.9455858e-05 -6.6116852e-05  1.3608603e-04 ... -1.5781938e-05\n",
      "   5.8481251e-05 -9.1547277e-05]\n",
      " [-5.8828846e-02  6.7614645e-02 -4.5764863e-02 ...  3.1541102e-02\n",
      "  -4.7071457e-02  1.8059494e-02]\n",
      " ...\n",
      " [ 1.8714922e-02 -3.4154747e-02 -5.7082116e-03 ...  9.0036495e-03\n",
      "  -5.3237779e-03 -3.6183164e-02]\n",
      " [ 8.1216330e-03 -1.5133495e-02 -8.5550675e-04 ... -3.6253810e-02\n",
      "  -1.3013718e-02 -1.7758729e-02]\n",
      " [ 3.4547005e-02 -3.0030517e-02 -1.4005583e-03 ... -2.2262277e-02\n",
      "   1.9773055e-02 -1.0151998e-02]]\n",
      "---\n",
      "238410\n"
     ]
    }
   ],
   "source": [
    "print(value1['seq2seq/embedding/embedding'])\n",
    "print (\"---\")\n",
    "print (value1['global_step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.8002606e-02  1.8546827e-02  1.3938937e-01 ... -4.9497869e-02\n",
      "   3.3019509e-02  3.5647683e-02]\n",
      " [ 7.9455858e-05 -6.6116852e-05  1.3608603e-04 ... -1.5781938e-05\n",
      "   5.8481251e-05 -9.1547277e-05]\n",
      " [-5.8828846e-02  6.7614645e-02 -4.5764863e-02 ...  3.1541102e-02\n",
      "  -4.7071457e-02  1.8059494e-02]\n",
      " ...\n",
      " [ 1.8714922e-02 -3.4154747e-02 -5.7082116e-03 ...  9.0036495e-03\n",
      "  -5.3237779e-03 -3.6183164e-02]\n",
      " [ 8.1216330e-03 -1.5133495e-02 -8.5550675e-04 ... -3.6253810e-02\n",
      "  -1.3013718e-02 -1.7758729e-02]\n",
      " [ 3.4547005e-02 -3.0030517e-02 -1.4005583e-03 ... -2.2262277e-02\n",
      "   1.9773055e-02 -1.0151998e-02]]\n",
      "---\n",
      "[[ 3.0038651e-05 -3.3294495e-05 -9.5765536e-05 ...  5.1459169e-06\n",
      "  -5.9785438e-05  4.0432846e-05]\n",
      " [ 1.5944187e-04 -1.8053758e-04  1.1748278e-05 ... -4.4734188e-05\n",
      "  -1.4305796e-04 -1.0234036e-04]\n",
      " [ 1.4953532e-04  1.2771579e-05  1.1298924e-04 ... -1.0402448e-04\n",
      "   1.2689688e-05 -4.4768585e-05]\n",
      " ...\n",
      " [-1.7323169e-04  2.2098213e-05  1.6521179e-05 ... -8.2679951e-05\n",
      "  -6.1318518e-05  4.1179388e-05]\n",
      " [ 1.8008008e-04 -7.8544203e-05  1.2364860e-04 ...  1.4127854e-04\n",
      "  -7.1354436e-05 -1.8327577e-04]\n",
      " [-2.9100311e-05 -3.6992351e-05 -1.1188567e-04 ...  6.7633970e-05\n",
      "  -9.2984570e-05 -3.8603353e-05]]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(value2['seq2seq/embedding/embedding'])\n",
    "print (\"---\")\n",
    "print(value2['seq2seq/embedding/embedding_doc'])\n",
    "print (value2['global_step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['seq2seq/reduce_final_st/w_reduce_h/Adagrad', 'seq2seq/reduce_final_st/w_reduce_c/Adagrad', 'seq2seq/reduce_final_st/w_reduce_c', 'seq2seq/reduce_final_st/bias_reduce_c/Adagrad', 'seq2seq/output_projection/w/Adagrad', 'seq2seq/output_projection/w', 'seq2seq/reduce_final_st/w_reduce_h', 'seq2seq/output_projection/v/Adagrad', 'seq2seq/decoder/attention_decoder/W_h/Adagrad', 'seq2seq/reduce_final_st/bias_reduce_c', 'seq2seq/decoder/attention_decoder/Linear/Matrix', 'seq2seq/decoder/attention_decoder/v', 'seq2seq/encoder/bidirectional_rnn/fw/lstm_cell/kernel/Adagrad', 'seq2seq/decoder/attention_decoder/Linear/Bias/Adagrad', 'seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Matrix', 'seq2seq/decoder/attention_decoder/W_h', 'seq2seq/decoder/attention_decoder/Linear/Matrix/Adagrad', 'seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Bias', 'seq2seq/decoder/attention_decoder/Linear/Bias', 'seq2seq/encoder/bidirectional_rnn/fw/lstm_cell/kernel', 'seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Bias/Adagrad', 'seq2seq/decoder/attention_decoder/Attention/Linear/Bias', 'seq2seq/encoder/bidirectional_rnn/bw/lstm_cell/kernel/Adagrad', 'seq2seq/reduce_final_st/bias_reduce_h', 'seq2seq/decoder/attention_decoder/Attention/Linear/Matrix', 'seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Matrix', 'global_step', 'seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Bias', 'seq2seq/encoder/bidirectional_rnn/bw/lstm_cell/bias/Adagrad', 'seq2seq/decoder/attention_decoder/Attention/Linear/Bias/Adagrad', 'seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Bias/Adagrad', 'seq2seq/decoder/attention_decoder/lstm_cell/kernel/Adagrad', 'seq2seq/output_projection/v', 'seq2seq/encoder/bidirectional_rnn/bw/lstm_cell/bias', 'seq2seq/decoder/attention_decoder/Attention/Linear/Matrix/Adagrad', 'seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Matrix/Adagrad', 'seq2seq/decoder/attention_decoder/lstm_cell/bias', 'seq2seq/decoder/attention_decoder/lstm_cell/bias/Adagrad', 'seq2seq/decoder/attention_decoder/lstm_cell/kernel', 'seq2seq/decoder/attention_decoder/v/Adagrad', 'seq2seq/embedding/embedding', 'seq2seq/embedding/embedding_doc/Adagrad', 'seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Matrix/Adagrad', 'seq2seq/encoder/bidirectional_rnn/fw/lstm_cell/bias/Adagrad', 'seq2seq/embedding/embedding_doc', 'seq2seq/embedding/embedding/Adagrad', 'seq2seq/encoder/bidirectional_rnn/bw/lstm_cell/kernel', 'seq2seq/reduce_final_st/bias_reduce_h/Adagrad', 'seq2seq/encoder/bidirectional_rnn/fw/lstm_cell/bias'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu3",
   "language": "python",
   "name": "nlu3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
