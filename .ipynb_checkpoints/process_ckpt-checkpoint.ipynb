{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "from tensorflow.python import pywrap_tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"/home/cs224u/pointer/log/test_exp2/train/\" + \"model.ckpt-0\"\n",
    "reader = tf.train.NewCheckpointReader(file_name)\n",
    "var_to_shape_map = reader.get_variable_to_shape_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "with tf.Session(config=config) as sess:\n",
    "    # define the new is_training tensor\n",
    "    embedding_doc = tf.random_normal([10, 128],dtype=tf.float32, name='embedding_doc')\n",
    "\n",
    "    # now import the graph using the .meta file of the checkpoint\n",
    "    saver = tf.train.import_meta_graph(\n",
    "    file_name+\".meta\", input_map={'seq2seq/embedding/embedding_doc:0':embedding_doc})\n",
    "\n",
    "    # restore all weights using the model checkpoint \n",
    "    saver.restore(sess, file_name)\n",
    "\n",
    "    # save updated graph and variables values\n",
    "    saver.save(sess, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finite = []\n",
    "all_infnan = []\n",
    "some_infnan = []\n",
    "\n",
    "for key in sorted(var_to_shape_map.keys()):\n",
    "    tensor = reader.get_tensor(key)\n",
    "    print (\"key: \", key)\n",
    "    print (tensor.shape)\n",
    "    if np.all(np.isfinite(tensor)):\n",
    "        finite.append(key)\n",
    "    else:\n",
    "        if not np.any(np.isfinite(tensor)):\n",
    "            all_infnan.append(key)\n",
    "        else:\n",
    "            some_infnan.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvalue(ckpt):\n",
    "    reader = pywrap_tensorflow.NewCheckpointReader(ckpt)\n",
    "    var_to_shape_map = reader.get_variable_to_shape_map()\n",
    "    value = {}\n",
    "    for key in var_to_shape_map:\n",
    "#         if \"Adagrad\" not in key:\n",
    "            value[key] = reader.get_tensor(key)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "# get value from checkpoint\n",
    "ckpt = '/home/cs224u/pointer/log/pretrained_model_tf1.2.1/train/model-238410'\n",
    "# ckpt = '/home/cs224u/pointer/log/test_exp/train/model.ckpt-0'\n",
    "value1 = getvalue(ckpt)\n",
    "print (len(value1))\n",
    "\n",
    "# ckpt = '/home/cs224u/pointer/log/docvec_ckpt_exp/train/model.ckpt-1'\n",
    "# ckpt = '/home/cs224u/pointer/log/ckpt/pointer_proj/model.ckpt-0'\n",
    "ckpt = '/home/cs224u/pointer/log/ckpt/multi_attn_2_proj/model.ckpt-0'\n",
    "value2 = getvalue(ckpt)\n",
    "print (len(value2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 128)\n",
      "---\n",
      "238410\n"
     ]
    }
   ],
   "source": [
    "print(value1['seq2seq/embedding/embedding'].shape)\n",
    "print (\"---\")\n",
    "print (value1['global_step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32)\n",
      "---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(value2['seq2seq/embedding/embedding'].shape)\n",
    "print (\"---\")\n",
    "# print(value2['seq2seq/embedding/embedding_doc'])\n",
    "print (value2['global_step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Bias\n",
      "seq2seq/decoder/attention_decoder/coverage/w_c\n",
      "seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Matrix\n",
      "seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Bias/Adagrad\n",
      "seq2seq/decoder/attention_decoder/calculate_pgen/Linear/Matrix/Adagrad\n",
      "seq2seq/decoder/attention_decoder/coverage/w_c/Adagrad\n",
      "---\n",
      "seq2seq/decoder/attention_decoder/calculate_pgen_0/Linear/Bias\n",
      "seq2seq/decoder/attention_decoder/calculate_pgen_1/Linear/Bias\n",
      "seq2seq/decoder/attention_decoder/calculate_pgen_1/Linear/Bias/Adagrad\n",
      "seq2seq/decoder/attention_decoder/calculate_pgen_0/Linear/Bias/Adagrad\n",
      "seq2seq/decoder/attention_decoder/calculate_pgen_0/Linear/Matrix\n",
      "seq2seq/decoder/attention_decoder/calculate_pgen_0/Linear/Matrix/Adagrad\n",
      "seq2seq/decoder/attention_decoder/calculate_pgen_1/Linear/Matrix\n",
      "seq2seq/decoder/attention_decoder/calculate_pgen_1/Linear/Matrix/Adagrad\n"
     ]
    }
   ],
   "source": [
    "for k in value1.keys():\n",
    "    if k not in value2:\n",
    "        print (k)\n",
    "        \n",
    "print (\"---\")\n",
    "\n",
    "for k in value2.keys():\n",
    "    if k not in value1:\n",
    "        print (k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['seq2seq/reduce_final_st/w_reduce_h/Adagrad', 'seq2seq/reduce_final_st/w_reduce_c/Adagrad', 'seq2seq/reduce_final_st/w_reduce_c', 'seq2seq/reduce_final_st/bias_reduce_c/Adagrad', 'seq2seq/output_projection/w/Adagrad', 'seq2seq/output_projection/w', 'seq2seq/reduce_final_st/w_reduce_h', 'seq2seq/output_projection/v/Adagrad', 'seq2seq/encoder/bidirectional_rnn/fw/lstm_cell/kernel', 'seq2seq/decoder/attention_decoder/calculate_pgen_0/Linear/Bias', 'seq2seq/decoder/attention_decoder/W_h/Adagrad', 'seq2seq/decoder/attention_decoder/calculate_pgen_1/Linear/Bias', 'seq2seq/reduce_final_st/bias_reduce_c', 'seq2seq/decoder/attention_decoder/Linear/Matrix', 'seq2seq/decoder/attention_decoder/v', 'seq2seq/encoder/bidirectional_rnn/fw/lstm_cell/kernel/Adagrad', 'seq2seq/decoder/attention_decoder/Linear/Bias/Adagrad', 'seq2seq/encoder/bidirectional_rnn/fw/lstm_cell/bias/Adagrad', 'seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Matrix/Adagrad', 'seq2seq/decoder/attention_decoder/calculate_pgen_1/Linear/Bias/Adagrad', 'seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Matrix', 'seq2seq/decoder/attention_decoder/Linear/Bias', 'seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Bias/Adagrad', 'seq2seq/decoder/attention_decoder/Attention/Linear/Bias', 'seq2seq/encoder/bidirectional_rnn/bw/lstm_cell/kernel/Adagrad', 'seq2seq/reduce_final_st/bias_reduce_h', 'seq2seq/decoder/attention_decoder/Attention/Linear/Matrix', 'global_step', 'seq2seq/decoder/attention_decoder/AttnOutputProjection/Linear/Bias', 'seq2seq/encoder/bidirectional_rnn/bw/lstm_cell/bias/Adagrad', 'seq2seq/decoder/attention_decoder/Attention/Linear/Bias/Adagrad', 'seq2seq/decoder/attention_decoder/lstm_cell/kernel/Adagrad', 'seq2seq/output_projection/v', 'seq2seq/encoder/bidirectional_rnn/bw/lstm_cell/bias', 'seq2seq/decoder/attention_decoder/Attention/Linear/Matrix/Adagrad', 'seq2seq/decoder/attention_decoder/calculate_pgen_0/Linear/Bias/Adagrad', 'seq2seq/decoder/attention_decoder/lstm_cell/kernel', 'seq2seq/decoder/attention_decoder/calculate_pgen_0/Linear/Matrix', 'seq2seq/decoder/attention_decoder/calculate_pgen_0/Linear/Matrix/Adagrad', 'seq2seq/decoder/attention_decoder/v/Adagrad', 'seq2seq/decoder/attention_decoder/calculate_pgen_1/Linear/Matrix', 'seq2seq/decoder/attention_decoder/W_h', 'seq2seq/decoder/attention_decoder/Linear/Matrix/Adagrad', 'seq2seq/decoder/attention_decoder/calculate_pgen_1/Linear/Matrix/Adagrad', 'seq2seq/decoder/attention_decoder/lstm_cell/bias', 'seq2seq/decoder/attention_decoder/lstm_cell/bias/Adagrad', 'seq2seq/embedding/embedding', 'seq2seq/embedding/embedding/Adagrad', 'seq2seq/encoder/bidirectional_rnn/bw/lstm_cell/kernel', 'seq2seq/reduce_final_st/bias_reduce_h/Adagrad', 'seq2seq/encoder/bidirectional_rnn/fw/lstm_cell/bias'])\n"
     ]
    }
   ],
   "source": [
    "print(value2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu3",
   "language": "python",
   "name": "nlu3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
